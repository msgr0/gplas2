Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	gplas_coocurr
	1	gplas_coverage
	1	gplas_paths
	1	mlplasmids
	4

[Mon Jun 13 14:23:16 2022]
Job 3: Running mlplasmids to obtain the plasmid prediction using the nodes extracted from the graph.

[Mon Jun 13 14:23:28 2022]
Finished job 3.
1 of 4 steps (25%) done

[Mon Jun 13 14:23:28 2022]
Job 2: Extracting the sd k-mer coverage from the chromosome-predicted contigs

[Mon Jun 13 14:23:28 2022]
Error in rule gplas_coverage:
    jobid: 2
    output: coverage/installation_estimation.txt, coverage/installation_graph_contigs.tab, coverage/installation_repeats_graph.tab, coverage/installation_clean_links.tab, coverage/installation_clean_prediction.tab, coverage/installation_initialize_nodes.tab

RuleException:
WorkflowError in line 98 of /home/rodrigo/gplas/gplas/snakefiles/mlplasmidssnake.smk:
URLError: <urlopen error [Errno 2] No such file or directory: '/home/rodrigo/gplas/gplas/snakefiles/gplas/scripts/gplas_coverage.R'>
  File "/home/rodrigo/gplas/gplas/snakefiles/mlplasmidssnake.smk", line 98, in __rule_gplas_coverage
  File "/home/rodrigo/miniconda3/envs/gplasdev/lib/python3.7/concurrent/futures/thread.py", line 57, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/rodrigo/gplas/.snakemake/log/2022-06-13T142316.456654.snakemake.log
